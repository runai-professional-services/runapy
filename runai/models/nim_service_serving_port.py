# coding: utf-8

"""
NVIDIA Run:ai

# Introduction  The NVIDIA Run:ai Control-Plane API reference is a guide that provides an easy-to-use programming interface for adding various tasks to your application, including workload submission, resource management, and administrative operations.  NVIDIA Run:ai APIs are accessed using *bearer tokens*. To obtain a token, you need to create a **Service account** through the NVIDIA Run:ai user interface. To create a service account, in your UI, go to Access → Service Accounts (for organization-level service accounts) or User settings → Access Keys (for user access keys), and create a new one.  After you have created a new service account, you will need to assign it access rules. To assign access rules to the service account, see [Create access rules](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/accessrules#create-or-delete-rules). Make sure you assign the correct rules to your service account. Use the [Roles](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/roles) to assign the correct access rules.  To get your access token, follow the instructions in [Request a token](https://run-ai-docs.nvidia.com/saas/reference/api/rest-auth/#request-an-api-token).

The version of the OpenAPI document: latest
Generated by OpenAPI Generator (https://openapi-generator.tech)

Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from runai.models.serving_port_exposed_protocol import ServingPortExposedProtocol
from runai.models.serving_port_service_type import ServingPortServiceType
from typing import Optional, Set
from typing_extensions import Self


class NimServiceServingPort(BaseModel):
    """
    Pydantic class model representing A port for accessing the inference service.

    Parameters:
        ```python
        service_type: Optional[ServingPortServiceType]
        port: Optional[int]
        grpc_port: Optional[int]
        metrics_port: Optional[int]
        expose_externally: Optional[bool]
        exposed_url: Optional[str]
        exposed_protocol: Optional[ServingPortExposedProtocol]
        ```
        service_type: See model ServingPortServiceType for more information. - Default: ServingPortServiceType.CLUSTERIP
        port: The port that the container running the inference service exposes (mandatory).
        grpc_port: The GRPC port that the container running the inference service exposes.
        metrics_port: The port where metrics are exposed, required only if it&#39;s different than the main port.
        expose_externally: Indicates whether the inference serving endpoint should be accessible outside the cluster. If set to true, the endpoint will be exposed externally. To enable external access, your administrator must configure the cluster as described in the [inference requirements](https://run-ai-docs.nvidia.com/saas/getting-started/installation/system-requirements#inference). section. - Default: True
        exposed_url: The custom URL to use for the serving port. If empty (default), an autogenerated URL will be used.
        exposed_protocol: See model ServingPortExposedProtocol for more information.
    Example:
        ```python
        NimServiceServingPort(
            service_type='ClusterIP',
                        port=8000,
                        grpc_port=8001,
                        metrics_port=8002,
                        expose_externally=True,
                        exposed_url='jUR,rZ#UM/?R,Fp^l6$ARj',
                        exposed_protocol='http'
        )
        ```
    """  # noqa: E501

    service_type: Optional[ServingPortServiceType] = Field(
        default=ServingPortServiceType.CLUSTERIP, alias="serviceType"
    )
    port: Optional[Annotated[int, Field(le=65535, strict=True, ge=1)]] = Field(
        default=None,
        description="The port that the container running the inference service exposes (mandatory).",
    )
    grpc_port: Optional[Annotated[int, Field(le=65535, strict=True, ge=1)]] = Field(
        default=None,
        description="The GRPC port that the container running the inference service exposes.",
        alias="grpcPort",
    )
    metrics_port: Optional[Annotated[int, Field(le=65535, strict=True, ge=1)]] = Field(
        default=None,
        description="The port where metrics are exposed, required only if it's different than the main port.",
        alias="metricsPort",
    )
    expose_externally: Optional[StrictBool] = Field(
        default=True,
        description="Indicates whether the inference serving endpoint should be accessible outside the cluster. If set to true, the endpoint will be exposed externally. To enable external access, your administrator must configure the cluster as described in the [inference requirements](https://run-ai-docs.nvidia.com/saas/getting-started/installation/system-requirements#inference). section.",
        alias="exposeExternally",
    )
    exposed_url: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="The custom URL to use for the serving port. If empty (default), an autogenerated URL will be used.",
        alias="exposedUrl",
    )
    exposed_protocol: Optional[ServingPortExposedProtocol] = Field(
        default=None, alias="exposedProtocol"
    )
    __properties: ClassVar[List[str]] = [
        "serviceType",
        "port",
        "grpcPort",
        "metricsPort",
        "exposeExternally",
        "exposedUrl",
        "exposedProtocol",
    ]

    @field_validator("exposed_url")
    def exposed_url_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of NimServiceServingPort from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # set to None if service_type (nullable) is None
        # and model_fields_set contains the field
        if self.service_type is None and "service_type" in self.model_fields_set:
            _dict["serviceType"] = None

        # set to None if port (nullable) is None
        # and model_fields_set contains the field
        if self.port is None and "port" in self.model_fields_set:
            _dict["port"] = None

        # set to None if grpc_port (nullable) is None
        # and model_fields_set contains the field
        if self.grpc_port is None and "grpc_port" in self.model_fields_set:
            _dict["grpcPort"] = None

        # set to None if metrics_port (nullable) is None
        # and model_fields_set contains the field
        if self.metrics_port is None and "metrics_port" in self.model_fields_set:
            _dict["metricsPort"] = None

        # set to None if expose_externally (nullable) is None
        # and model_fields_set contains the field
        if (
            self.expose_externally is None
            and "expose_externally" in self.model_fields_set
        ):
            _dict["exposeExternally"] = None

        # set to None if exposed_url (nullable) is None
        # and model_fields_set contains the field
        if self.exposed_url is None and "exposed_url" in self.model_fields_set:
            _dict["exposedUrl"] = None

        # set to None if exposed_protocol (nullable) is None
        # and model_fields_set contains the field
        if (
            self.exposed_protocol is None
            and "exposed_protocol" in self.model_fields_set
        ):
            _dict["exposedProtocol"] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of NimServiceServingPort from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "serviceType": (
                    obj.get("serviceType")
                    if obj.get("serviceType") is not None
                    else ServingPortServiceType.CLUSTERIP
                ),
                "port": obj.get("port"),
                "grpcPort": obj.get("grpcPort"),
                "metricsPort": obj.get("metricsPort"),
                "exposeExternally": (
                    obj.get("exposeExternally")
                    if obj.get("exposeExternally") is not None
                    else True
                ),
                "exposedUrl": obj.get("exposedUrl"),
                "exposedProtocol": obj.get("exposedProtocol"),
            }
        )
        return _obj
