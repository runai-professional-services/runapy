# coding: utf-8

"""
NVIDIA Run:ai

# Introduction  The NVIDIA Run:ai Control-Plane API reference is a guide that provides an easy-to-use programming interface for adding various tasks to your application, including workload submission, resource management, and administrative operations.  NVIDIA Run:ai APIs are accessed using *bearer tokens*. To obtain a token, you need to create a **Service account** through the NVIDIA Run:ai user interface. To create a service account, in your UI, go to Access → Service Accounts (for organization-level service accounts) or User settings → Access Keys (for user access keys), and create a new one.  After you have created a new service account, you will need to assign it access rules. To assign access rules to the service account, see [Create access rules](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/accessrules#create-or-delete-rules). Make sure you assign the correct rules to your service account. Use the [Roles](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/roles) to assign the correct access rules.  To get your access token, follow the instructions in [Request a token](https://run-ai-docs.nvidia.com/saas/reference/api/rest-auth/#request-an-api-token).

The version of the OpenAPI document: latest
Generated by OpenAPI Generator (https://openapi-generator.tech)

Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, field_validator
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from runai.models.gpu_request_type import GpuRequestType
from typing import Optional, Set
from typing_extensions import Self


class NimServiceDefaultsAllOfCompute(BaseModel):
    """
    Pydantic class model representing NimServiceDefaultsAllOfCompute.

    Parameters:
        ```python
        cpu_core_limit: Optional[float]
        cpu_core_request: Optional[float]
        cpu_memory_limit: Optional[str]
        cpu_memory_request: Optional[str]
        gpu_devices_request: Optional[int]
        gpu_memory_limit: Optional[str]
        gpu_memory_request: Optional[str]
        gpu_portion_limit: Optional[float]
        gpu_portion_request: Optional[float]
        gpu_request_type: Optional[GpuRequestType]
        ```
        cpu_core_limit: Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs.
        cpu_core_request: CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload.
        cpu_memory_limit: Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit.
        cpu_memory_request: The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload
        gpu_devices_request: Requested number of GPU devices. Currently if more than one device is requested, it is not possible to provide values for gpuMemory or gpuPortion.
        gpu_memory_limit: Limitation on the memory consumed by the workload, per GPU device. The system guarantees The gpuMemoryLimit must be no less than gpuMemoryRequest.
        gpu_memory_request: Required if and only if gpuRequestType is memory. States the GPU memory to allocate for the created workload, per GPU device. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload.
        gpu_portion_limit: Limitations on the portion consumed by the workload, per GPU device. The system guarantees The gpuPotionLimit must be no less than the gpuPortionRequest.
        gpu_portion_request: Required if and only if gpuRequestType is portion. States the portion of the GPU to allocate for the created workload, per GPU device, between 0 and 1. The default is no allocated GPUs.
        gpu_request_type: See model GpuRequestType for more information.
    Example:
        ```python
        NimServiceDefaultsAllOfCompute(
            cpu_core_limit=2,
                        cpu_core_request=0.5,
                        cpu_memory_limit='30M',
                        cpu_memory_request='20M',
                        gpu_devices_request=1,
                        gpu_memory_limit='10M',
                        gpu_memory_request='10M',
                        gpu_portion_limit=0.5,
                        gpu_portion_request=0.5,
                        gpu_request_type='portion'
        )
        ```
    """  # noqa: E501

    cpu_core_limit: Optional[
        Union[
            Annotated[float, Field(strict=True, ge=0)],
            Annotated[int, Field(strict=True, ge=0)],
        ]
    ] = Field(
        default=None,
        description="Limitations on the number of CPUs consumed by the workload (0.5, 1, .etc). The system guarantees that this workload will not be able to consume more than this amount of CPUs.",
        alias="cpuCoreLimit",
    )
    cpu_core_request: Optional[
        Union[
            Annotated[float, Field(strict=True, ge=0)],
            Annotated[int, Field(strict=True, ge=0)],
        ]
    ] = Field(
        default=None,
        description="CPU units to allocate for the created workload (0.5, 1, .etc). The workload will receive at least this amount of CPU. Note that the workload will not be scheduled unless the system can guarantee this amount of CPUs to the workload.",
        alias="cpuCoreRequest",
    )
    cpu_memory_limit: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="Limitations on the CPU memory to allocate for this workload (1G, 20M, .etc). The system guarantees that this workload will not be able to consume more than this amount of memory. The workload will receive an error when trying to allocate more memory than this limit.",
        alias="cpuMemoryLimit",
    )
    cpu_memory_request: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="The amount of CPU memory to allocate for this workload (1G, 20M, .etc). The workload will receive at least this amount of memory. Note that the workload will not be scheduled unless the system can guarantee this amount of memory to the workload",
        alias="cpuMemoryRequest",
    )
    gpu_devices_request: Optional[Annotated[int, Field(strict=True, ge=0)]] = Field(
        default=None,
        description="Requested number of GPU devices. Currently if more than one device is requested, it is not possible to provide values for gpuMemory or gpuPortion.",
        alias="gpuDevicesRequest",
    )
    gpu_memory_limit: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="Limitation on the memory consumed by the workload, per GPU device. The system guarantees The gpuMemoryLimit must be no less than gpuMemoryRequest.",
        alias="gpuMemoryLimit",
    )
    gpu_memory_request: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="Required if and only if gpuRequestType is memory. States the GPU memory to allocate for the created workload, per GPU device. Note that the workload will not be scheduled unless the system can guarantee this amount of GPU memory to the workload.",
        alias="gpuMemoryRequest",
    )
    gpu_portion_limit: Optional[
        Union[
            Annotated[float, Field(strict=True, ge=0)],
            Annotated[int, Field(strict=True, ge=0)],
        ]
    ] = Field(
        default=None,
        description="Limitations on the portion consumed by the workload, per GPU device. The system guarantees The gpuPotionLimit must be no less than the gpuPortionRequest.",
        alias="gpuPortionLimit",
    )
    gpu_portion_request: Optional[
        Union[
            Annotated[float, Field(strict=True, ge=0)],
            Annotated[int, Field(strict=True, ge=0)],
        ]
    ] = Field(
        default=None,
        description="Required if and only if gpuRequestType is portion. States the portion of the GPU to allocate for the created workload, per GPU device, between 0 and 1. The default is no allocated GPUs.",
        alias="gpuPortionRequest",
    )
    gpu_request_type: Optional[GpuRequestType] = Field(
        default=None, alias="gpuRequestType"
    )
    __properties: ClassVar[List[str]] = [
        "cpuCoreLimit",
        "cpuCoreRequest",
        "cpuMemoryLimit",
        "cpuMemoryRequest",
        "gpuDevicesRequest",
        "gpuMemoryLimit",
        "gpuMemoryRequest",
        "gpuPortionLimit",
        "gpuPortionRequest",
        "gpuRequestType",
    ]

    @field_validator("cpu_memory_limit")
    def cpu_memory_limit_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r"^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$", value):
            raise ValueError(
                r"must validate the regular expression /^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$/"
            )
        return value

    @field_validator("cpu_memory_request")
    def cpu_memory_request_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r"^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$", value):
            raise ValueError(
                r"must validate the regular expression /^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$/"
            )
        return value

    @field_validator("gpu_memory_limit")
    def gpu_memory_limit_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r"^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$", value):
            raise ValueError(
                r"must validate the regular expression /^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$/"
            )
        return value

    @field_validator("gpu_memory_request")
    def gpu_memory_request_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r"^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$", value):
            raise ValueError(
                r"must validate the regular expression /^([+]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$/"
            )
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of NimServiceDefaultsAllOfCompute from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # set to None if cpu_core_limit (nullable) is None
        # and model_fields_set contains the field
        if self.cpu_core_limit is None and "cpu_core_limit" in self.model_fields_set:
            _dict["cpuCoreLimit"] = None

        # set to None if cpu_core_request (nullable) is None
        # and model_fields_set contains the field
        if (
            self.cpu_core_request is None
            and "cpu_core_request" in self.model_fields_set
        ):
            _dict["cpuCoreRequest"] = None

        # set to None if cpu_memory_limit (nullable) is None
        # and model_fields_set contains the field
        if (
            self.cpu_memory_limit is None
            and "cpu_memory_limit" in self.model_fields_set
        ):
            _dict["cpuMemoryLimit"] = None

        # set to None if cpu_memory_request (nullable) is None
        # and model_fields_set contains the field
        if (
            self.cpu_memory_request is None
            and "cpu_memory_request" in self.model_fields_set
        ):
            _dict["cpuMemoryRequest"] = None

        # set to None if gpu_devices_request (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_devices_request is None
            and "gpu_devices_request" in self.model_fields_set
        ):
            _dict["gpuDevicesRequest"] = None

        # set to None if gpu_memory_limit (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_memory_limit is None
            and "gpu_memory_limit" in self.model_fields_set
        ):
            _dict["gpuMemoryLimit"] = None

        # set to None if gpu_memory_request (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_memory_request is None
            and "gpu_memory_request" in self.model_fields_set
        ):
            _dict["gpuMemoryRequest"] = None

        # set to None if gpu_portion_limit (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_portion_limit is None
            and "gpu_portion_limit" in self.model_fields_set
        ):
            _dict["gpuPortionLimit"] = None

        # set to None if gpu_portion_request (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_portion_request is None
            and "gpu_portion_request" in self.model_fields_set
        ):
            _dict["gpuPortionRequest"] = None

        # set to None if gpu_request_type (nullable) is None
        # and model_fields_set contains the field
        if (
            self.gpu_request_type is None
            and "gpu_request_type" in self.model_fields_set
        ):
            _dict["gpuRequestType"] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of NimServiceDefaultsAllOfCompute from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "cpuCoreLimit": obj.get("cpuCoreLimit"),
                "cpuCoreRequest": obj.get("cpuCoreRequest"),
                "cpuMemoryLimit": obj.get("cpuMemoryLimit"),
                "cpuMemoryRequest": obj.get("cpuMemoryRequest"),
                "gpuDevicesRequest": obj.get("gpuDevicesRequest"),
                "gpuMemoryLimit": obj.get("gpuMemoryLimit"),
                "gpuMemoryRequest": obj.get("gpuMemoryRequest"),
                "gpuPortionLimit": obj.get("gpuPortionLimit"),
                "gpuPortionRequest": obj.get("gpuPortionRequest"),
                "gpuRequestType": obj.get("gpuRequestType"),
            }
        )
        return _obj
